{"cells":[{"cell_type":"markdown","metadata":{"id":"3VoAzV4gj4Cq"},"source":["# 1. Setup and Get Data"]},{"cell_type":"markdown","metadata":{"id":"DbG3UFWkj4Cs"},"source":["### 1.1 Install Dependencies and Setup"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"15JgaUf4j4Cs","executionInfo":{"status":"ok","timestamp":1719322018657,"user_tz":-180,"elapsed":18932,"user":{"displayName":"Ahmad Omar","userId":"13750794922665820237"}},"outputId":"887d163b-06e3-4c55-b131-dc75f5ca1c01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting labelme\n","  Using cached labelme-5.5.0.tar.gz (1.4 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from labelme) (5.1.0)\n","Collecting imgviz>=1.7.5 (from labelme)\n","  Downloading imgviz-1.7.5.tar.gz (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from labelme) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from labelme) (1.25.2)\n","Collecting onnxruntime!=1.16.0,>=1.14.1 (from labelme)\n","  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=2.8 in /usr/local/lib/python3.10/dist-packages (from labelme) (9.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from labelme) (6.0.1)\n","Collecting qtpy!=1.11.2 (from labelme)\n","  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from labelme) (0.19.3)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from labelme) (2.4.0)\n","Collecting PyQt5!=5.15.3,!=5.15.4 (from labelme)\n","  Downloading PyQt5-5.15.10-cp37-abi3-manylinux_2_17_x86_64.whl (8.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n","Collecting coloredlogs (from onnxruntime!=1.16.0,>=1.14.1->labelme)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime!=1.16.0,>=1.14.1->labelme) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime!=1.16.0,>=1.14.1->labelme) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime!=1.16.0,>=1.14.1->labelme) (1.12.1)\n","Collecting PyQt5-sip<13,>=12.13 (from PyQt5!=5.15.3,!=5.15.4->labelme)\n","  Downloading PyQt5_sip-12.13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl (338 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.1/338.1 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyQt5-Qt5>=5.15.2 (from PyQt5!=5.15.3,!=5.15.4->labelme)\n","  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.12.2)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->labelme) (3.3)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->labelme) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->labelme) (2024.6.18)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->labelme) (1.6.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->labelme) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->labelme) (3.15.3)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->labelme) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown->labelme) (4.66.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.5.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->labelme) (2.5)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.16.0,>=1.14.1->labelme)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->labelme) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->labelme) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->labelme) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->labelme) (2024.6.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->labelme) (1.7.1)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime!=1.16.0,>=1.14.1->labelme) (1.3.0)\n","Building wheels for collected packages: labelme, imgviz\n","  Building wheel for labelme (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for labelme: filename=labelme-5.5.0-py3-none-any.whl size=1437464 sha256=badf9e5097f79d98d4c2c1a1f0cba60b7f7cfc423004922bc8261c98a27c1805\n","  Stored in directory: /root/.cache/pip/wheels/44/3a/5e/487d2768ccb5f2411334ac0f3fa7265c94decca9fd94f64d6b\n","  Building wheel for imgviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgviz: filename=imgviz-1.7.5-py3-none-any.whl size=7680232 sha256=7f37409e868aa0bd29b46dc1a03d27257c2e1a9870cf8fdcef345b1483e8c66b\n","  Stored in directory: /root/.cache/pip/wheels/01/1b/cb/3c1f12e177813f5ed105b1e1452a88ed3e39744bdb30cac625\n","Successfully built labelme imgviz\n","Installing collected packages: PyQt5-Qt5, qtpy, PyQt5-sip, humanfriendly, PyQt5, coloredlogs, onnxruntime, imgviz, labelme\n","Successfully installed PyQt5-5.15.10 PyQt5-Qt5-5.15.2 PyQt5-sip-12.13.0 coloredlogs-15.0.1 humanfriendly-10.0 imgviz-1.7.5 labelme-5.5.0 onnxruntime-1.18.0 qtpy-2.4.1\n"]}],"source":["# Install necessary packages excluding tensorflow-gpu\n","!pip install labelme opencv-python matplotlib albumentations"]},{"cell_type":"markdown","metadata":{"id":"ZoFzp18Yj4Ct"},"source":["### 1.2 Collect Images Using OpenCV"]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[],"id":"HLPnDjsij4Ct","executionInfo":{"status":"ok","timestamp":1719322089969,"user_tz":-180,"elapsed":956,"user":{"displayName":"Ahmad Omar","userId":"13750794922665820237"}}},"outputs":[],"source":["import os\n","import time\n","import uuid\n","import cv2"]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[],"id":"-ookTBrUj4Cu","executionInfo":{"status":"ok","timestamp":1719322202633,"user_tz":-180,"elapsed":528,"user":{"displayName":"Ahmad Omar","userId":"13750794922665820237"}}},"outputs":[],"source":["IMAGES_PATH = os.path.join('data','images')\n","number_images = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ySNVx2WMj4Cu"},"outputs":[],"source":["cap = cv2.VideoCapture(1)\n","for imgnum in range(number_images):\n","    print('Collecting image {}'.format(imgnum))\n","    ret, frame = cap.read()\n","    imgname = os.path.join(IMAGES_PATH,f'{str(uuid.uuid1())}.jpg')\n","    cv2.imwrite(imgname, frame)\n","    cv2.imshow('frame', frame)\n","    time.sleep(0.5)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"TyCrhRAYj4Cu"},"source":["### 1.3 Annotate Images with LabelMe"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"5HtKH3-lj4Cu"},"outputs":[],"source":["!labelme"]},{"cell_type":"markdown","metadata":{"id":"sYIDvR0Cj4Cv"},"source":["# 2. Review Dataset and Build Image Loading Function"]},{"cell_type":"markdown","metadata":{"id":"MeYUfvN4j4Cv"},"source":["### 2.1 Import TF and Deps"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"kNIvFM3Fj4Cv"},"outputs":[],"source":["import tensorflow as tf\n","import json\n","import numpy as np\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"49yjmDD0j4Cv"},"source":["### 2.2 Limit GPU Memory Growth"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"EmJtOIFzj4Cv"},"outputs":[],"source":["# Avoid OOM errors by setting GPU Memory Consumption Growth\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"asnO30Taj4Cw"},"outputs":[],"source":["tf.config.list_physical_devices('GPU')"]},{"cell_type":"markdown","metadata":{"id":"xyvf1uR2j4Cw"},"source":["### 2.3 Load Image into TF Data Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"0gue6AXMj4Cw"},"outputs":[],"source":["images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eELDFnz8j4Cw"},"outputs":[],"source":["images.as_numpy_iterator().next()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"7ylX4pjKj4Cw"},"outputs":[],"source":["def load_image(x):\n","    byte_img = tf.io.read_file(x)\n","    img = tf.io.decode_jpeg(byte_img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"zaSB8A8sj4Cx"},"outputs":[],"source":["images = images.map(load_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"I4Yd9cygj4Cx"},"outputs":[],"source":["images.as_numpy_iterator().next()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTgxG9-Lj4Cx"},"outputs":[],"source":["type(images)"]},{"cell_type":"markdown","metadata":{"id":"p7s6O7_sj4Cx"},"source":["### 2.4 View Raw Images with Matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"bwTpyeohj4Cx"},"outputs":[],"source":["image_generator = images.batch(4).as_numpy_iterator()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"CP1HOEF-j4Cx"},"outputs":[],"source":["plot_images = image_generator.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"LxCkSmh_j4Cx"},"outputs":[],"source":["fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n","for idx, image in enumerate(plot_images):\n","    ax[idx].imshow(image)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1dtMJKHYj4Cy"},"source":["# 3. Partition Unaugmented Data"]},{"cell_type":"markdown","metadata":{"id":"l99x0-Anj4Cy"},"source":["### 3.1 MANUALLY SPLT DATA INTO TRAIN TEST AND VAL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFODTet4j4Cy"},"outputs":[],"source":["90*.7 # 63 to train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1gqAHB0j4Cy"},"outputs":[],"source":["90*.15 # 14 and 13 to test and val"]},{"cell_type":"markdown","metadata":{"id":"w4PJvwHGj4Cy"},"source":["### 3.2 Move the Matching Labels"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"KOQf-Vxkj4Cz"},"outputs":[],"source":["for folder in ['train','test','val']:\n","    for file in os.listdir(os.path.join('data', folder, 'images')):\n","\n","        filename = file.split('.')[0]+'.json'\n","        existing_filepath = os.path.join('data','labels', filename)\n","        if os.path.exists(existing_filepath):\n","            new_filepath = os.path.join('data',folder,'labels',filename)\n","            os.replace(existing_filepath, new_filepath)"]},{"cell_type":"markdown","metadata":{"id":"78mpxso-j4C0"},"source":["# 4. Apply Image Augmentation on Images and Labels using Albumentations"]},{"cell_type":"markdown","metadata":{"id":"c5ltPdUdj4C0"},"source":["### 4.1 Setup Albumentations Transform Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"fgpNKj4Dj4C1"},"outputs":[],"source":["import albumentations as alb"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"mRyopX-Sj4C1"},"outputs":[],"source":["augmentor = alb.Compose([alb.RandomCrop(width=450, height=450),\n","                         alb.HorizontalFlip(p=0.5),\n","                         alb.RandomBrightnessContrast(p=0.2),\n","                         alb.RandomGamma(p=0.2),\n","                         alb.RGBShift(p=0.2),\n","                         alb.VerticalFlip(p=0.5)],\n","                       bbox_params=alb.BboxParams(format='albumentations',\n","                                                  label_fields=['class_labels']))"]},{"cell_type":"markdown","metadata":{"id":"UNaGOBr4j4C1"},"source":["### 4.2 Load a Test Image and Annotation with OpenCV and JSON"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"R3yZ1Syyj4C1"},"outputs":[],"source":["img = cv2.imread(os.path.join('data','train', 'images','ffd85fc5-cc1a-11ec-bfb8-a0cec8d2d278.jpg'))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ThQ1oUL3j4C2"},"outputs":[],"source":["with open(os.path.join('data', 'train', 'labels', 'ffd85fc5-cc1a-11ec-bfb8-a0cec8d2d278.json'), 'r') as f:\n","    label = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZliSYujEj4C3"},"outputs":[],"source":["label['shapes'][0]['points']"]},{"cell_type":"markdown","metadata":{"id":"6htCaFDaj4C3"},"source":["### 4.3 Extract Coordinates and Rescale to Match Image Resolution"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"1z8gXcEOj4C3"},"outputs":[],"source":["coords = [0,0,0,0]\n","coords[0] = label['shapes'][0]['points'][0][0]\n","coords[1] = label['shapes'][0]['points'][0][1]\n","coords[2] = label['shapes'][0]['points'][1][0]\n","coords[3] = label['shapes'][0]['points'][1][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0lpb7Atdj4C3"},"outputs":[],"source":["coords"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"TWp32zZKj4C3"},"outputs":[],"source":["coords = list(np.divide(coords, [640,480,640,480]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1OILSYEj4C4"},"outputs":[],"source":["coords"]},{"cell_type":"markdown","metadata":{"id":"6DZzIto4j4C4"},"source":["### 4.4 Apply Augmentations and View Results"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ptzcD6TUj4C4"},"outputs":[],"source":["augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"WTytqZF7j4C4"},"outputs":[],"source":["augmented['bboxes'][0][2:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_8cXPZhj4C4"},"outputs":[],"source":["augmented['bboxes']"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"PyanzDyXj4C4"},"outputs":[],"source":["cv2.rectangle(augmented['image'],\n","              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n","              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)),\n","                    (255,0,0), 2)\n","\n","plt.imshow(augmented['image'])"]},{"cell_type":"markdown","metadata":{"id":"s4eZpZYQj4C4"},"source":["# 5. Build and Run Augmentation Pipeline"]},{"cell_type":"markdown","metadata":{"id":"7hLcv9cZj4C4"},"source":["### 5.1 Run Augmentation Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"CV4l3J-aj4C4"},"outputs":[],"source":["for partition in ['train','test','val']:\n","    for image in os.listdir(os.path.join('data', partition, 'images')):\n","        img = cv2.imread(os.path.join('data', partition, 'images', image))\n","\n","        coords = [0,0,0.00001,0.00001]\n","        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                label = json.load(f)\n","\n","            coords[0] = label['shapes'][0]['points'][0][0]\n","            coords[1] = label['shapes'][0]['points'][0][1]\n","            coords[2] = label['shapes'][0]['points'][1][0]\n","            coords[3] = label['shapes'][0]['points'][1][1]\n","            coords = list(np.divide(coords, [640,480,640,480]))\n","\n","        try:\n","            for x in range(60):\n","                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n","                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n","\n","                annotation = {}\n","                annotation['image'] = image\n","\n","                if os.path.exists(label_path):\n","                    if len(augmented['bboxes']) == 0:\n","                        annotation['bbox'] = [0,0,0,0]\n","                        annotation['class'] = 0\n","                    else:\n","                        annotation['bbox'] = augmented['bboxes'][0]\n","                        annotation['class'] = 1\n","                else:\n","                    annotation['bbox'] = [0,0,0,0]\n","                    annotation['class'] = 0\n","\n","\n","                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n","                    json.dump(annotation, f)\n","\n","        except Exception as e:\n","            print(e)"]},{"cell_type":"markdown","metadata":{"id":"VQRt7sf6j4C4"},"source":["### 5.2 Load Augmented Images to Tensorflow Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"zKGtnIODj4C5"},"outputs":[],"source":["train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n","train_images = train_images.map(load_image)\n","train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n","train_images = train_images.map(lambda x: x/255)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"thad0fp_j4C5"},"outputs":[],"source":["test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n","test_images = test_images.map(load_image)\n","test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n","test_images = test_images.map(lambda x: x/255)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"GzY-FFDHj4C5"},"outputs":[],"source":["val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n","val_images = val_images.map(load_image)\n","val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n","val_images = val_images.map(lambda x: x/255)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"jQNj0Itmj4C5"},"outputs":[],"source":["train_images.as_numpy_iterator().next()"]},{"cell_type":"markdown","metadata":{"id":"bvb8qGXYj4C5"},"source":["# 6. Prepare Labels"]},{"cell_type":"markdown","metadata":{"id":"9PdNQPjpj4C5"},"source":["### 6.1 Build Label Loading Function"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"qOkthvoDj4C5"},"outputs":[],"source":["def load_labels(label_path):\n","    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n","        label = json.load(f)\n","\n","    return [label['class']], label['bbox']"]},{"cell_type":"markdown","metadata":{"id":"O35Ru5cmj4C5"},"source":["### 6.2 Load Labels to Tensorflow Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"AWzVqimej4C5"},"outputs":[],"source":["train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n","train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"391JHLOtj4C5"},"outputs":[],"source":["test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n","test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"jsWsW0D7j4C5"},"outputs":[],"source":["val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n","val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wIrqgivj4C6"},"outputs":[],"source":["train_labels.as_numpy_iterator().next()"]},{"cell_type":"markdown","metadata":{"id":"X76B40bJj4C6"},"source":["# 7. Combine Label and Image Samples"]},{"cell_type":"markdown","metadata":{"id":"WAw8WlhHj4C6"},"source":["### 7.1 Check Partition Lengths"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"P47XgCwqj4C6"},"outputs":[],"source":["len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"]},{"cell_type":"markdown","metadata":{"id":"5S2hjsiEj4C6"},"source":["### 7.2 Create Final Datasets (Images/Labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-j-dBLK2j4C6"},"outputs":[],"source":["train = tf.data.Dataset.zip((train_images, train_labels))\n","train = train.shuffle(5000)\n","train = train.batch(8)\n","train = train.prefetch(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_xsRI5QSj4C6"},"outputs":[],"source":["test = tf.data.Dataset.zip((test_images, test_labels))\n","test = test.shuffle(1300)\n","test = test.batch(8)\n","test = test.prefetch(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"jxd3mbwhj4C6"},"outputs":[],"source":["val = tf.data.Dataset.zip((val_images, val_labels))\n","val = val.shuffle(1000)\n","val = val.batch(8)\n","val = val.prefetch(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"cqlUNMPmj4C6"},"outputs":[],"source":["train.as_numpy_iterator().next()[1]"]},{"cell_type":"markdown","metadata":{"id":"F9rtA9Udj4C6"},"source":["### 7.3 View Images and Annotations"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"PU6leSQAj4C6"},"outputs":[],"source":["data_samples = train.as_numpy_iterator()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"yTXib6OLj4C6"},"outputs":[],"source":["res = data_samples.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"4dIE8qZLj4C7"},"outputs":[],"source":["fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n","for idx in range(4):\n","    sample_image = res[0][idx]\n","    sample_coords = res[1][1][idx]\n","\n","    cv2.rectangle(sample_image,\n","                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n","                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)),\n","                        (255,0,0), 2)\n","\n","    ax[idx].imshow(sample_image)"]},{"cell_type":"markdown","metadata":{"id":"3JZ7BsuWj4C7"},"source":["# 8. Build Deep Learning using the Functional API"]},{"cell_type":"markdown","metadata":{"id":"eROKWu_Jj4C7"},"source":["### 8.1 Import Layers and Base Network"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"eJnZwv7gj4C7"},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n","from tensorflow.keras.applications import VGG16"]},{"cell_type":"markdown","metadata":{"id":"RHVnTDSnj4C7"},"source":["### 8.2 Download VGG16"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"tJH6absgj4C7"},"outputs":[],"source":["vgg = VGG16(include_top=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"TjUX-zXbj4C7"},"outputs":[],"source":["vgg.summary()"]},{"cell_type":"markdown","metadata":{"id":"fEFl10zSj4C7"},"source":["### 8.3 Build instance of Network"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"lNDmudFvj4C7"},"outputs":[],"source":["def build_model():\n","    input_layer = Input(shape=(120,120,3))\n","\n","    vgg = VGG16(include_top=False)(input_layer)\n","\n","    # Classification Model\n","    f1 = GlobalMaxPooling2D()(vgg)\n","    class1 = Dense(2048, activation='relu')(f1)\n","    class2 = Dense(1, activation='sigmoid')(class1)\n","\n","    # Bounding box model\n","    f2 = GlobalMaxPooling2D()(vgg)\n","    regress1 = Dense(2048, activation='relu')(f2)\n","    regress2 = Dense(4, activation='sigmoid')(regress1)\n","\n","    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n","    return facetracker"]},{"cell_type":"markdown","metadata":{"id":"SUWV6Zsdj4C7"},"source":["### 8.4 Test out Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"JZlhbaiFj4C7"},"outputs":[],"source":["facetracker = build_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"de91LjSAj4C7"},"outputs":[],"source":["facetracker.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"qkkjw8Xoj4C7"},"outputs":[],"source":["X, y = train.as_numpy_iterator().next()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"9lRRNUnHj4C7"},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"u0-TH8w4j4C8"},"outputs":[],"source":["classes, coords = facetracker.predict(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"bKF6XORRj4C8"},"outputs":[],"source":["classes, coords"]},{"cell_type":"markdown","metadata":{"id":"ebS3qXxFj4C8"},"source":["# 9. Define Losses and Optimizers"]},{"cell_type":"markdown","metadata":{"id":"-NWCYaFCj4C8"},"source":["### 9.1 Define Optimizer and LR"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"mYmWL3QKj4C8"},"outputs":[],"source":["batches_per_epoch = len(train)\n","lr_decay = (1./0.75 -1)/batches_per_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"5OtG65VRj4C8"},"outputs":[],"source":["opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"]},{"cell_type":"markdown","metadata":{"id":"fhzWkCXxj4C8"},"source":["### 9.2 Create Localization Loss and Classification Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"HhKqtEyij4C8"},"outputs":[],"source":["def localization_loss(y_true, yhat):\n","    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n","\n","    h_true = y_true[:,3] - y_true[:,1]\n","    w_true = y_true[:,2] - y_true[:,0]\n","\n","    h_pred = yhat[:,3] - yhat[:,1]\n","    w_pred = yhat[:,2] - yhat[:,0]\n","\n","    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n","\n","    return delta_coord + delta_size"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"NRAJ56K8j4C8"},"outputs":[],"source":["classloss = tf.keras.losses.BinaryCrossentropy()\n","regressloss = localization_loss"]},{"cell_type":"markdown","metadata":{"id":"NTZcWU_3j4C8"},"source":["### 9.3 Test out Loss Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"JUiSxJQnj4C8"},"outputs":[],"source":["localization_loss(y[1], coords)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"yHykKu9Ej4C8"},"outputs":[],"source":["classloss(y[0], classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Ofd6sIZRj4C9"},"outputs":[],"source":["regressloss(y[1], coords)"]},{"cell_type":"markdown","metadata":{"id":"Tu2PYDnJj4C9"},"source":["# 10. Train Neural Network"]},{"cell_type":"markdown","metadata":{"id":"V4XwqKfsj4C9"},"source":["### 10.1 Create Custom Model Class"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"fBoDmXJJj4C9"},"outputs":[],"source":["class FaceTracker(Model):\n","    def __init__(self, eyetracker,  **kwargs):\n","        super().__init__(**kwargs)\n","        self.model = eyetracker\n","\n","    def compile(self, opt, classloss, localizationloss, **kwargs):\n","        super().compile(**kwargs)\n","        self.closs = classloss\n","        self.lloss = localizationloss\n","        self.opt = opt\n","\n","    def train_step(self, batch, **kwargs):\n","\n","        X, y = batch\n","\n","        with tf.GradientTape() as tape:\n","            classes, coords = self.model(X, training=True)\n","\n","            batch_classloss = self.closs(y[0], classes)\n","            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n","\n","            total_loss = batch_localizationloss+0.5*batch_classloss\n","\n","            grad = tape.gradient(total_loss, self.model.trainable_variables)\n","\n","        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n","\n","        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n","\n","    def test_step(self, batch, **kwargs):\n","        X, y = batch\n","\n","        classes, coords = self.model(X, training=False)\n","\n","        batch_classloss = self.closs(y[0], classes)\n","        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n","        total_loss = batch_localizationloss+0.5*batch_classloss\n","\n","        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n","\n","    def call(self, X, **kwargs):\n","        return self.model(X, **kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"tBcY-Yk-j4C9"},"outputs":[],"source":["model = FaceTracker(facetracker)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"hImlHhBTj4C9"},"outputs":[],"source":["model.compile(opt, classloss, regressloss)"]},{"cell_type":"markdown","metadata":{"id":"tEotSySij4C9"},"source":["### 10.2 Train"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_7Kped6oj4C9"},"outputs":[],"source":["logdir='logs'"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"qmpJfPuMj4C9"},"outputs":[],"source":["tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"0-MObI6Mj4C9"},"outputs":[],"source":["hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"]},{"cell_type":"markdown","metadata":{"id":"mwuShwpcj4C9"},"source":["### 10.3 Plot Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"HEPC9kEYj4C9"},"outputs":[],"source":["hist.history"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"efVTJFhFj4C9"},"outputs":[],"source":["fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n","\n","ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n","ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n","ax[0].title.set_text('Loss')\n","ax[0].legend()\n","\n","ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n","ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n","ax[1].title.set_text('Classification Loss')\n","ax[1].legend()\n","\n","ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n","ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n","ax[2].title.set_text('Regression Loss')\n","ax[2].legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_udnPEsOj4C9"},"source":["# 11. Make Predictions"]},{"cell_type":"markdown","metadata":{"id":"BVkod1e3j4C-"},"source":["### 11.1 Make Predictions on Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"75JZhDtrj4C-"},"outputs":[],"source":["test_data = test.as_numpy_iterator()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"gpW7N_CNj4C-"},"outputs":[],"source":["test_sample = test_data.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"WbhPUI8rj4C-"},"outputs":[],"source":["yhat = facetracker.predict(test_sample[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"HSsojjrpj4C-"},"outputs":[],"source":["fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n","for idx in range(4):\n","    sample_image = test_sample[0][idx]\n","    sample_coords = yhat[1][idx]\n","\n","    if yhat[0][idx] > 0.9:\n","        cv2.rectangle(sample_image,\n","                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n","                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)),\n","                            (255,0,0), 2)\n","\n","    ax[idx].imshow(sample_image)"]},{"cell_type":"markdown","metadata":{"id":"XtWSTvj1j4C-"},"source":["### 11.2 Save the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Wc56Y80nj4C-"},"outputs":[],"source":["from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"klYREiZ1j4C-"},"outputs":[],"source":["facetracker.save('facetracker.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"WYz3CwSJj4C-"},"outputs":[],"source":["facetracker = load_model('facetracker.h5')"]},{"cell_type":"markdown","metadata":{"id":"vSE4lN31j4C-"},"source":["### 11.3 Real Time Detection"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"inamx0HGj4C-"},"outputs":[],"source":["cap = cv2.VideoCapture(1)\n","while cap.isOpened():\n","    _ , frame = cap.read()\n","    frame = frame[50:500, 50:500,:]\n","\n","    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    resized = tf.image.resize(rgb, (120,120))\n","\n","    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n","    sample_coords = yhat[1][0]\n","\n","    if yhat[0] > 0.5:\n","        # Controls the main rectangle\n","        cv2.rectangle(frame,\n","                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n","                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)),\n","                            (255,0,0), 2)\n","        # Controls the label rectangle\n","        cv2.rectangle(frame,\n","                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n","                                    [0,-30])),\n","                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n","                                    [80,0])),\n","                            (255,0,0), -1)\n","\n","        # Controls the text rendered\n","        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n","                                               [0,-5])),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n","\n","    cv2.imshow('EyeTrack', frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUjjRGgrj4C-"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"facedet","language":"python","name":"facedet"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}